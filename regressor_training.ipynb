{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyPugM1bqJ3tF7upWj6lydN5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Algonauts Challenge\n","\n","Modified algonauts' challenge template for training VAE-based model"],"metadata":{"id":"h1FZcH0kz_4V"}},{"cell_type":"markdown","source":["## Environment configuraton"],"metadata":{"id":"rhslM6-B0va4"}},{"cell_type":"markdown","source":["### User parameters"],"metadata":{"id":"VOnxGXHYY3Tw"}},{"cell_type":"code","source":["from traitlets.traitlets import ForwardDeclaredInstance\n","\n","rand_seed = 5 #@param {allow-input: true}\n","\n","platform = 'colab' #@param ['colab', 'jupyter_notebook'] {allow-input: true}\n","\n","device = 'cuda' #@param ['cpu', 'cuda'] {allow-input: true}\n","\n","subj = 6 #@param [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"] {type:\"raw\", allow-input: true}\n","\n","batch_size = 200 #@param {type:\"integer\", allow-input: true}\n","\n","latent_dim = 100 #@param {type:\"integer\", allow-input: true}\n","\n","regressor = 'linear' #@param ['linear', 'mlp'] {allow-input: true}\n","\n","vae_model_path = '/NeuroAI/vae_100' #@param {type:\"string\", allow-input: true}\n","\n","regressor_model_path = '/NeuroAI/linear_s6' #@param {type:\"string\", allow-input: true}\n","\n","write_out_regressor = False #@param {type:\"boolean\", allow-input: true}"],"metadata":{"id":"LGk0D6-v5pJ4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Set up environment"],"metadata":{"id":"u58Vh6QwY59p"}},{"cell_type":"code","source":["!git clone https://github.com/AntixK/PyTorch-VAE"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Aw-NekzAFdiY","executionInfo":{"status":"ok","timestamp":1680561089537,"user_tz":240,"elapsed":10,"user":{"displayName":"Jackson Cornell","userId":"11282324665534091904"}},"outputId":"55d2a67d-ee15-4c1b-fa96-54036eb52aa4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'PyTorch-VAE' already exists and is not an empty directory.\n"]}]},{"cell_type":"code","source":["!cd PyTorch-VAE"],"metadata":{"id":"ybmsakk4GJXk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -r PyTorch-VAE/requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aYXOA88kGNdA","executionInfo":{"status":"ok","timestamp":1680561094766,"user_tz":240,"elapsed":5233,"user":{"displayName":"Jackson Cornell","userId":"11282324665534091904"}},"outputId":"b441eb07-bfdf-4228-f44e-6d674d9d2abc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pytorch-lightning==1.5.6 in /usr/local/lib/python3.9/dist-packages (from -r PyTorch-VAE/requirements.txt (line 1)) (1.5.6)\n","Requirement already satisfied: PyYAML==6.0 in /usr/local/lib/python3.9/dist-packages (from -r PyTorch-VAE/requirements.txt (line 2)) (6.0)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from -r PyTorch-VAE/requirements.txt (line 3)) (2.12.0)\n","Requirement already satisfied: torch>=1.6.1 in /usr/local/lib/python3.9/dist-packages (from -r PyTorch-VAE/requirements.txt (line 4)) (2.0.0+cu118)\n","Requirement already satisfied: torchsummary==1.5.1 in /usr/local/lib/python3.9/dist-packages (from -r PyTorch-VAE/requirements.txt (line 5)) (1.5.1)\n","Requirement already satisfied: torchvision>=0.10.1 in /usr/local/lib/python3.9/dist-packages (from -r PyTorch-VAE/requirements.txt (line 6)) (0.15.1+cu118)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.5.6->-r PyTorch-VAE/requirements.txt (line 1)) (4.65.0)\n","Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.5.6->-r PyTorch-VAE/requirements.txt (line 1)) (0.18.3)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.5.6->-r PyTorch-VAE/requirements.txt (line 1)) (23.0)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.5.6->-r PyTorch-VAE/requirements.txt (line 1)) (1.22.4)\n","Requirement already satisfied: pyDeprecate==0.3.1 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.5.6->-r PyTorch-VAE/requirements.txt (line 1)) (0.3.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.5.6->-r PyTorch-VAE/requirements.txt (line 1)) (4.5.0)\n","Requirement already satisfied: torchmetrics>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.5.6->-r PyTorch-VAE/requirements.txt (line 1)) (0.11.4)\n","Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.5.6->-r PyTorch-VAE/requirements.txt (line 1)) (2023.3.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (3.4.3)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (0.7.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (0.40.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (67.6.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (2.27.1)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (1.53.0)\n","Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (3.20.3)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (2.17.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (1.4.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (1.8.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (2.2.3)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (0.4.6)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.1->-r PyTorch-VAE/requirements.txt (line 4)) (3.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.1->-r PyTorch-VAE/requirements.txt (line 4)) (1.11.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.1->-r PyTorch-VAE/requirements.txt (line 4)) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.1->-r PyTorch-VAE/requirements.txt (line 4)) (2.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.1->-r PyTorch-VAE/requirements.txt (line 4)) (3.10.7)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.1->-r PyTorch-VAE/requirements.txt (line 4)) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.1->-r PyTorch-VAE/requirements.txt (line 4)) (16.0.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision>=0.10.1->-r PyTorch-VAE/requirements.txt (line 6)) (8.4.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.9/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.6->-r PyTorch-VAE/requirements.txt (line 1)) (3.8.4)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (5.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (4.9)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (0.2.8)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (1.16.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (6.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (2.0.12)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.6.1->-r PyTorch-VAE/requirements.txt (line 4)) (1.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.6->-r PyTorch-VAE/requirements.txt (line 1)) (1.3.3)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.6->-r PyTorch-VAE/requirements.txt (line 1)) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.6->-r PyTorch-VAE/requirements.txt (line 1)) (1.8.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.6->-r PyTorch-VAE/requirements.txt (line 1)) (22.2.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.6->-r PyTorch-VAE/requirements.txt (line 1)) (4.0.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.6->-r PyTorch-VAE/requirements.txt (line 1)) (1.3.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (3.15.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (3.2.2)\n"]}]},{"cell_type":"code","source":["import sys\n","sys.path.insert(0, '/content/PyTorch-VAE')"],"metadata":{"id":"CVIws9cbHPkq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import numpy as np\n","from pathlib import Path\n","from PIL import Image\n","from tqdm import tqdm\n","import matplotlib\n","from matplotlib import pyplot as plt\n","\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n","from torchvision import transforms\n","from scipy.stats import pearsonr as corr\n","from torchsummary import summary\n","\n","from models import VanillaVAE"],"metadata":{"id":"PuuLUMZh1Zf_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if platform == 'colab':\n","    from google.colab import drive\n","    drive.mount('/content/drive/', force_remount=True)\n","    data_dir = '/content/drive/MyDrive/algonauts_2023_tutorial_data' #@param {type:\"string\"}\n","    parent_submission_dir = '/content/drive/MyDrive/algonauts_2023_challenge_submission' #@param {type:\"string\"}\n","elif platform == 'jupyter_notebook':\n","    data_dir = '../algonauts_2023_challenge_data'\n","    parent_submission_dir = '../algonauts_2023_challenge_submission'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E1j8NTSDOzEU","executionInfo":{"status":"ok","timestamp":1680561101176,"user_tz":240,"elapsed":5910,"user":{"displayName":"Jackson Cornell","userId":"11282324665534091904"}},"outputId":"5ab3a327-7691-4583-be4e-d7fec0860299"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","source":["device = 'cuda' #@param ['cpu', 'cuda'] {allow-input: true}\n","device = torch.device(device)"],"metadata":{"id":"XOYmh5NqP-FW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class argObj:\n","  def __init__(self, data_dir, parent_submission_dir, subj):\n","    \n","    self.subj = format(subj, '02')\n","    self.data_dir = os.path.join(data_dir, 'subj'+self.subj)\n","    self.parent_submission_dir = parent_submission_dir\n","    self.subject_submission_dir = os.path.join(self.parent_submission_dir,\n","        'subj'+self.subj)\n","\n","    # Create the submission directory if not existing\n","    if not os.path.isdir(self.subject_submission_dir):\n","        os.makedirs(self.subject_submission_dir)\n","\n","args = argObj(data_dir, parent_submission_dir, subj)"],"metadata":{"id":"XwHWzqmc1M1M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Preprocess data"],"metadata":{"id":"IlFj3_EW0ZZp"}},{"cell_type":"markdown","source":["### Load voxel data"],"metadata":{"id":"xgI2baY6LcwX"}},{"cell_type":"code","source":["fmri_dir = os.path.join(args.data_dir, 'training_split', 'training_fmri')\n","lh_fmri = np.load(os.path.join(fmri_dir, 'lh_training_fmri.npy'))\n","rh_fmri = np.load(os.path.join(fmri_dir, 'rh_training_fmri.npy'))\n","\n","print('LH training fMRI data shape:')\n","print(lh_fmri.shape)\n","print('(Training stimulus images × LH vertices)')\n","\n","print('\\nRH training fMRI data shape:')\n","print(rh_fmri.shape)\n","print('(Training stimulus images × RH vertices)')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"biFldRj3LhE8","executionInfo":{"status":"ok","timestamp":1680561131774,"user_tz":240,"elapsed":30601,"user":{"displayName":"Jackson Cornell","userId":"11282324665534091904"}},"outputId":"ecde51f7-a69e-4b30-d55f-6018cad68f12"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["LH training fMRI data shape:\n","(9082, 18978)\n","(Training stimulus images × LH vertices)\n","\n","RH training fMRI data shape:\n","(9082, 20220)\n","(Training stimulus images × RH vertices)\n"]}]},{"cell_type":"markdown","source":["### Load images"],"metadata":{"id":"Ozloaer713dX"}},{"cell_type":"code","source":["train_img_dir  = os.path.join(args.data_dir, 'training_split', 'training_images')\n","test_img_dir  = os.path.join(args.data_dir, 'test_split', 'test_images')\n","\n","# Create lists will all training and test image file names, sorted\n","train_img_list = os.listdir(train_img_dir)\n","train_img_list.sort()\n","test_img_list = os.listdir(test_img_dir)\n","test_img_list.sort()\n","print('Training images: ' + str(len(train_img_list)))\n","print('Test images: ' + str(len(test_img_list)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1m5C9nkl1rhp","executionInfo":{"status":"ok","timestamp":1680561171001,"user_tz":240,"elapsed":39231,"user":{"displayName":"Jackson Cornell","userId":"11282324665534091904"}},"outputId":"44e61f1c-ecac-4f14-ae49-20098f7b8ed2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training images: 9082\n","Test images: 293\n"]}]},{"cell_type":"code","source":["train_img_file = train_img_list[0]\n","print('Training image file name: ' + train_img_file)\n","print('73k NSD images ID: ' + train_img_file[-9:-4])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-cAWKqIX12M6","executionInfo":{"status":"ok","timestamp":1680561171002,"user_tz":240,"elapsed":20,"user":{"displayName":"Jackson Cornell","userId":"11282324665534091904"}},"outputId":"bf9b42dd-a0d3-4de8-968c-f911793186cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training image file name: train-0001_nsd-00004.png\n","73k NSD images ID: 00004\n"]}]},{"cell_type":"markdown","source":["### Spit into train/test"],"metadata":{"id":"U9e1e7rR2b8j"}},{"cell_type":"code","source":["np.random.seed(rand_seed)\n","\n","# Calculate how many stimulus images correspond to 90% of the training data\n","num_train = int(np.round(len(train_img_list) / 100 * 90))\n","# Shuffle all training stimulus images\n","idxs = np.arange(len(train_img_list))\n","np.random.shuffle(idxs)\n","# Assign 90% of the shuffled stimulus images to the training partition,\n","# and 10% to the test partition\n","idxs_train, idxs_val = idxs[:num_train], idxs[num_train:]\n","# No need to shuffle or split the test stimulus images\n","idxs_test = np.arange(len(test_img_list))\n","\n","print('Training stimulus images: ' + format(len(idxs_train)))\n","print('\\nValidation stimulus images: ' + format(len(idxs_val)))\n","print('\\nTest stimulus images: ' + format(len(idxs_test)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XUKSQMLq2bNs","executionInfo":{"status":"ok","timestamp":1680561171003,"user_tz":240,"elapsed":17,"user":{"displayName":"Jackson Cornell","userId":"11282324665534091904"}},"outputId":"d9d57c44-ecb1-4e35-9197-d542ebf5b634"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training stimulus images: 8174\n","\n","Validation stimulus images: 908\n","\n","Test stimulus images: 293\n"]}]},{"cell_type":"markdown","source":["### Create data pipeline"],"metadata":{"id":"skVNGKEB3iIC"}},{"cell_type":"code","source":["transform = transforms.Compose([\n","    transforms.Resize((224, 224)), # resize the images to 224x224 pixels\n","    transforms.ToTensor(), # convert the images to a PyTorch tensor\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalize the images color channels\n","])"],"metadata":{"id":"hnIRSE2b3IwB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Define class for easy image manipulation"],"metadata":{"id":"zA3VLc8i3qwi"}},{"cell_type":"code","source":["class ImageDataset(Dataset):\n","    def __init__(self, imgs_paths, idxs, transform):\n","        self.imgs_paths = np.array(imgs_paths)[idxs]\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.imgs_paths)\n","\n","    def __getitem__(self, idx):\n","        # Load the image\n","        img_path = self.imgs_paths[idx]\n","        img = Image.open(img_path).convert('RGB')\n","        # Preprocess the image and send it to the chosen device ('cpu' or 'cuda')\n","        if self.transform:\n","            img = self.transform(img).to(device)\n","        return img"],"metadata":{"id":"QOiO6McC3wzq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the paths of all image files\n","train_imgs_paths = sorted(list(Path(train_img_dir).iterdir()))\n","test_imgs_paths = sorted(list(Path(test_img_dir).iterdir()))\n","\n","# The DataLoaders contain the ImageDataset class\n","train_imgs_dataloader = DataLoader(\n","    ImageDataset(train_imgs_paths, idxs_train, transform), \n","    batch_size=batch_size\n",")\n","val_imgs_dataloader = DataLoader(\n","    ImageDataset(train_imgs_paths, idxs_val, transform), \n","    batch_size=batch_size\n",")\n","test_imgs_dataloader = DataLoader(\n","    ImageDataset(test_imgs_paths, idxs_test, transform), \n","    batch_size=batch_size\n",")"],"metadata":{"id":"K9fKJI7YKNaA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Split fMRI data"],"metadata":{"id":"KPWEwh3qMR5i"}},{"cell_type":"code","source":["lh_fmri_train = lh_fmri[idxs_train]\n","lh_fmri_val = lh_fmri[idxs_val]\n","rh_fmri_train = rh_fmri[idxs_train]\n","rh_fmri_val = rh_fmri[idxs_val]\n","\n","del lh_fmri, rh_fmri"],"metadata":{"id":"lfZ_l-iMMSDZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Autoencoder"],"metadata":{"id":"KtZJptfUMblK"}},{"cell_type":"markdown","source":["### Get encoder"],"metadata":{"id":"ur-lEb4WMeSQ"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)\n","vae = torch.load(os.path.join('/content/drive/MyDrive/', vae_model_path))\n","vae.to(device)\n","\n","train_nodes, _ = get_graph_node_names(vae)\n","print(train_nodes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KRPfOLNcDQNK","executionInfo":{"status":"ok","timestamp":1680561178056,"user_tz":240,"elapsed":6707,"user":{"displayName":"Jackson Cornell","userId":"11282324665534091904"}},"outputId":"9993da4f-e1b2-4ec8-a9f8-8cab3017b4f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n","['input', '_kwargs', 'encoder.0.0', 'encoder.0.1', 'encoder.0.2', 'encoder.1.0', 'encoder.1.1', 'encoder.1.2', 'encoder.2.0', 'encoder.2.1', 'encoder.2.2', 'encoder.3.0', 'encoder.3.1', 'encoder.3.2', 'encoder.4.0', 'encoder.4.1', 'encoder.4.2', 'flatten', 'fc_mu', 'fc_var', 'mul', 'exp', 'randn_like', 'mul_1', 'add', 'decoder_input', 'view', 'decoder.0.0', 'decoder.0.1', 'decoder.0.2', 'decoder.1.0', 'decoder.1.1', 'decoder.1.2', 'decoder.2.0', 'decoder.2.1', 'decoder.2.2', 'decoder.3.0', 'decoder.3.1', 'decoder.3.2', 'final_layer.0', 'final_layer.1', 'final_layer.2', 'final_layer.3', 'final_layer.4']\n"]}]},{"cell_type":"code","source":["vae.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YvtcMbPIMKJC","executionInfo":{"status":"ok","timestamp":1680561178057,"user_tz":240,"elapsed":20,"user":{"displayName":"Jackson Cornell","userId":"11282324665534091904"}},"outputId":"d1e04629-a6e3-47e3-fed2-5359e18b17cd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["VanillaVAE(\n","  (encoder): Sequential(\n","    (0): Sequential(\n","      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): LeakyReLU(negative_slope=0.01)\n","    )\n","    (1): Sequential(\n","      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): LeakyReLU(negative_slope=0.01)\n","    )\n","    (2): Sequential(\n","      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): LeakyReLU(negative_slope=0.01)\n","    )\n","    (3): Sequential(\n","      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): LeakyReLU(negative_slope=0.01)\n","    )\n","    (4): Sequential(\n","      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): LeakyReLU(negative_slope=0.01)\n","    )\n","  )\n","  (fc_mu): Linear(in_features=2048, out_features=100, bias=True)\n","  (fc_var): Linear(in_features=2048, out_features=100, bias=True)\n","  (decoder_input): Linear(in_features=100, out_features=2048, bias=True)\n","  (decoder): Sequential(\n","    (0): Sequential(\n","      (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n","      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): LeakyReLU(negative_slope=0.01)\n","    )\n","    (1): Sequential(\n","      (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n","      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): LeakyReLU(negative_slope=0.01)\n","    )\n","    (2): Sequential(\n","      (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n","      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): LeakyReLU(negative_slope=0.01)\n","    )\n","    (3): Sequential(\n","      (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n","      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): LeakyReLU(negative_slope=0.01)\n","    )\n","  )\n","  (final_layer): Sequential(\n","    (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n","    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01)\n","    (3): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (4): Tanh()\n","  )\n",")"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["### Construct feature extractor"],"metadata":{"id":"RGBMbraxKUGV"}},{"cell_type":"code","source":["encoder_mu = 'fc_mu'\n","\n","feature_extractor = create_feature_extractor(vae, return_nodes=[encoder_mu])"],"metadata":{"id":"XJ9DgoYoMDXh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model training"],"metadata":{"id":"bukbt7JG6Q4y"}},{"cell_type":"markdown","source":["### Extract features"],"metadata":{"id":"1vQsdvTkNvSe"}},{"cell_type":"code","source":["def extract_features(feature_extractor, dataloader):\n","\n","    features = []\n","    for _, d in tqdm(enumerate(dataloader), total=len(dataloader)):\n","        # downsample data\n","        downsampled = torch.nn.functional.interpolate(d, size=[64, 64], mode='bilinear')\n","        # extract features\n","        ft = feature_extractor(downsampled)\n","        # Flatten the features\n","        ft = torch.hstack([torch.flatten(l, start_dim=1) for l in ft.values()])\n","        # add to features list\n","        features.append(ft)\n","        \n","    return np.vstack(features)"],"metadata":{"id":"YNEsjxg3Nxgu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["features_train = extract_features(feature_extractor, train_imgs_dataloader)\n","features_val = extract_features(feature_extractor, val_imgs_dataloader)\n","features_test = extract_features(feature_extractor, test_imgs_dataloader)\n","\n","print('\\nTraining images features:')\n","print(features_train.shape)\n","print('(Training stimulus images × features)')\n","\n","print('\\nValidation images features:')\n","print(features_val.shape)\n","print('(Validation stimulus images × features)')\n","\n","print('\\nTest images features:')\n","print(features_val.shape)\n","print('(Test stimulus images × features)')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_rPZ6x12ODuh","executionInfo":{"status":"ok","timestamp":1680569495885,"user_tz":240,"elapsed":8317843,"user":{"displayName":"Jackson Cornell","userId":"11282324665534091904"}},"outputId":"69cb28b9-77da-43af-a50a-2d445bafce22"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 41/41 [2:00:59<00:00, 177.06s/it]\n","100%|██████████| 5/5 [13:17<00:00, 159.54s/it]\n","100%|██████████| 2/2 [04:20<00:00, 130.08s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Training images features:\n","(41, 1)\n","(Training stimulus images × features)\n","\n","Validation images features:\n","(5, 1)\n","(Validation stimulus images × features)\n","\n","Test images features:\n","(5, 1)\n","(Test stimulus images × features)\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["### Define regressor"],"metadata":{"id":"hjfTNtq5NWH5"}},{"cell_type":"code","source":["class LinearRegression(torch.nn.Module):\n","    def __init__(self, inputSize, outputSize):\n","        super(LinearRegression, self).__init__()\n","        self.linear = torch.nn.Linear(inputSize, outputSize)\n","\n","    def forward(self, x):\n","        out = self.linear(x)\n","        return out"],"metadata":{"id":"AEwC7MAdwyTO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MLP(torch.nn.Module):\n","    def __init__(self, inputSize, outputSize):\n","        super(MLP, self).__init__()\n","        self.linear = torch.nn.Linear(inputSize, outputSize)\n","\n","    def forward(self, x):\n","        out = self.linear(x)\n","        return out"],"metadata":{"id":"9c9oAP4kxHoN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if (regressor == 'linear'):\n","    reg_lh = LinearRegression(latent_dim, lh_fmri_train.shape[1])\n","    reg_rh = LinearRegression(latent_dim, rh_fmri_train.shape[1])\n","elif (regressor == 'mlp'):\n","    reg_lh = MLP(latent_dim, lh_fmri_train.shape[1])\n","    reg_rh = MLP(latent_dim, rh_fmri_train.shape[1])\n","\n","reg_lh.to(device)\n","reg_rh.to(device)\n","\n","print(reg_lh.eval())\n","print(reg_rh.eval())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o8nVaWLbySG4","executionInfo":{"status":"ok","timestamp":1680570655305,"user_tz":240,"elapsed":5,"user":{"displayName":"Jackson Cornell","userId":"11282324665534091904"}},"outputId":"60a602c5-e144-4a70-d590-407e39d1d054"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["LinearRegression(\n","  (linear): Linear(in_features=100, out_features=18978, bias=True)\n",")\n","LinearRegression(\n","  (linear): Linear(in_features=100, out_features=20220, bias=True)\n",")\n"]}]},{"cell_type":"markdown","source":["### Train regressor"],"metadata":{"id":"sllw8SOpyMSt"}},{"cell_type":"code","source":["def Training(model, features, outputs, epochs=1, batch_size=200):\n","\n","    # parameters\n","    lr = 0.005\n","\n","    # error criterion\n","    criterion = torch.nn.MSELoss() \n","\n","    # optimizer\n","    optimizer = torch.optim.Adam(model.parameters(),\n","                               lr=lr,\n","                               weight_decay=1e-5)\n","    \n","    # calculate batches\n","    batch_iter = int(features.shape[1] / batch_size) + 1\n","    \n","    for b in range(batch_iter):\n","\n","        # get current batch\n","        x = torch.tensor(features[:, (b * batch_size):(((b+1) * batch_size))])\n","        y = torch.tensor(outputs[:, (b * batch_size):(((b+1) * batch_size))])\n","\n","        # add to same device as model\n","        x.to(device)\n","        y.to(device)\n","\n","        for i in range(epochs):\n","\n","            # predict output\n","            y_hat = model(x)\n","\n","            # calculate loss\n","            loss = criterion(y_hat, y)\n","\n","            # calculate gradient\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            # display progress\n","            print('\\t training loss: %f' % (loss))"],"metadata":{"id":"Q3pFBeku0Vos"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train left hemisphere regressor\n","Training(reg_lh, features_train, lh_fmri_train)"],"metadata":{"id":"p2mf50ym5H8y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train right hemisphere regressor\n","Training(reg_rh, features_train, rh_fmri_train)"],"metadata":{"id":"9PuuVb8d5OLk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use fitted linear regressions to predict the validation and test fMRI data\n","lh_fmri_val_pred = reg_lh(features_val)\n","lh_fmri_test_pred = reg_lh(features_test)\n","rh_fmri_val_pred = reg_rh(features_val)\n","rh_fmri_test_pred = reg_rh(features_test)"],"metadata":{"id":"kVPJvCsSNYWd","executionInfo":{"status":"error","timestamp":1680569598929,"user_tz":240,"elapsed":7,"user":{"displayName":"Jackson Cornell","userId":"11282324665534091904"}},"colab":{"base_uri":"https://localhost:8080/","height":235},"outputId":"bf99f34f-12bd-49c9-b3eb-64a05ca47f6f"},"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-802f36470332>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# # Fit linear regressions on the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mreg_lh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlh_fmri_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mreg_rh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrh_fmri_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'shape'"]}]},{"cell_type":"markdown","source":["## Evaluate results"],"metadata":{"id":"BakN9tVdNg0Y"}},{"cell_type":"markdown","source":["### Compute encoding accuracy with Pearson's correlation"],"metadata":{"id":"kqKccdlnNn0h"}},{"cell_type":"code","source":["# Empty correlation array of shape: (LH vertices)\n","lh_correlation = np.zeros(lh_fmri_val_pred.shape[1])\n","# Correlate each predicted LH vertex with the corresponding ground truth vertex\n","for v in tqdm(range(lh_fmri_val_pred.shape[1])):\n","    lh_correlation[v] = corr(lh_fmri_val_pred[:,v], lh_fmri_val[:,v])[0]\n","\n","# Empty correlation array of shape: (RH vertices)\n","rh_correlation = np.zeros(rh_fmri_val_pred.shape[1])\n","# Correlate each predicted RH vertex with the corresponding ground truth vertex\n","for v in tqdm(range(rh_fmri_val_pred.shape[1])):\n","    rh_correlation[v] = corr(rh_fmri_val_pred[:,v], rh_fmri_val[:,v])[0]"],"metadata":{"id":"L5TdVJaLNjvT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Visualize encoding accuracy"],"metadata":{"id":"Vb5lnlHuN1ni"}},{"cell_type":"code","source":["# Load the ROI classes mapping dictionaries\n","roi_mapping_files = ['mapping_prf-visualrois.npy', 'mapping_floc-bodies.npy',\n","    'mapping_floc-faces.npy', 'mapping_floc-places.npy',\n","    'mapping_floc-words.npy', 'mapping_streams.npy']\n","roi_name_maps = []\n","for r in roi_mapping_files:\n","    roi_name_maps.append(np.load(os.path.join(args.data_dir, 'roi_masks', r),\n","        allow_pickle=True).item())\n","\n","# Load the ROI brain surface maps\n","lh_challenge_roi_files = ['lh.prf-visualrois_challenge_space.npy',\n","    'lh.floc-bodies_challenge_space.npy', 'lh.floc-faces_challenge_space.npy',\n","    'lh.floc-places_challenge_space.npy', 'lh.floc-words_challenge_space.npy',\n","    'lh.streams_challenge_space.npy']\n","rh_challenge_roi_files = ['rh.prf-visualrois_challenge_space.npy',\n","    'rh.floc-bodies_challenge_space.npy', 'rh.floc-faces_challenge_space.npy',\n","    'rh.floc-places_challenge_space.npy', 'rh.floc-words_challenge_space.npy',\n","    'rh.streams_challenge_space.npy']\n","lh_challenge_rois = []\n","rh_challenge_rois = []\n","for r in range(len(lh_challenge_roi_files)):\n","    lh_challenge_rois.append(np.load(os.path.join(args.data_dir, 'roi_masks',\n","        lh_challenge_roi_files[r])))\n","    rh_challenge_rois.append(np.load(os.path.join(args.data_dir, 'roi_masks',\n","        rh_challenge_roi_files[r])))\n","\n","# Select the correlation results vertices of each ROI\n","roi_names = []\n","lh_roi_correlation = []\n","rh_roi_correlation = []\n","for r1 in range(len(lh_challenge_rois)):\n","    for r2 in roi_name_maps[r1].items():\n","        if r2[0] != 0: # zeros indicate to vertices falling outside the ROI of interest\n","            roi_names.append(r2[1])\n","            lh_roi_idx = np.where(lh_challenge_rois[r1] == r2[0])[0]\n","            rh_roi_idx = np.where(rh_challenge_rois[r1] == r2[0])[0]\n","            lh_roi_correlation.append(lh_correlation[lh_roi_idx])\n","            rh_roi_correlation.append(rh_correlation[rh_roi_idx])\n","roi_names.append('All vertices')\n","lh_roi_correlation.append(lh_correlation)\n","rh_roi_correlation.append(rh_correlation)\n","\n","# Create the plot\n","lh_median_roi_correlation = [np.median(lh_roi_correlation[r])\n","    for r in range(len(lh_roi_correlation))]\n","rh_median_roi_correlation = [np.median(rh_roi_correlation[r])\n","    for r in range(len(rh_roi_correlation))]\n","plt.figure(figsize=(18,6))\n","x = np.arange(len(roi_names))\n","width = 0.30\n","plt.bar(x - width/2, lh_median_roi_correlation, width, label='Left Hemisphere')\n","plt.bar(x + width/2, rh_median_roi_correlation, width,\n","    label='Right Hemishpere')\n","plt.xlim(left=min(x)-.5, right=max(x)+.5)\n","plt.ylim(bottom=0, top=1)\n","plt.xlabel('ROIs')\n","plt.xticks(ticks=x, labels=roi_names, rotation=60)\n","plt.ylabel('Median Pearson\\'s $r$')\n","plt.legend(frameon=True, loc=1);"],"metadata":{"id":"yCi81C_lN1DA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Submit results"],"metadata":{"id":"29WzATZHN9nU"}},{"cell_type":"code","source":["lh_fmri_test_pred = lh_fmri_test_pred.astype(np.float32)\n","rh_fmri_test_pred = rh_fmri_test_pred.astype(np.float32)"],"metadata":{"id":"BGr45pcXOBQU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.save(os.path.join(args.subject_submission_dir, 'lh_pred_test.npy'), lh_fmri_test_pred)\n","np.save(os.path.join(args.subject_submission_dir, 'rh_pred_test.npy'), rh_fmri_test_pred)"],"metadata":{"id":"-dODrccDODgd"},"execution_count":null,"outputs":[]}]}