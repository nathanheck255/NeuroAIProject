{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyM4N8LgWxuaAcYvHDsUZtyF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"markdown","source":["# Algonauts Challenge\n","\n","Modified algonauts' challenge template for training VAE-based model"],"metadata":{"id":"h1FZcH0kz_4V"}},{"cell_type":"markdown","source":["## Environment configuraton"],"metadata":{"id":"rhslM6-B0va4"}},{"cell_type":"markdown","source":["### User parameters"],"metadata":{"id":"VOnxGXHYY3Tw"}},{"cell_type":"code","source":["rand_seed = 5 #@param {allow-input: true}\n","\n","platform = 'colab' #@param ['colab', 'jupyter_notebook'] {allow-input: true}\n","\n","device = 'cuda' #@param ['cpu', 'cuda'] {allow-input: true}\n","\n","subj = 6 #@param [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"] {type:\"raw\", allow-input: true}\n","\n","batch_size = 64 #@param {type:\"integer\", allow-input: true}\n","\n","latent_dim = 100 #@param {type:\"integer\", allow-input: true}\n","\n","retrain = True #@param {type:\"boolean\", allow-input: true}\n","\n","model_path = '/NeuroAI/vae_100' #@param {type:\"string\", allow-input: true}"],"metadata":{"id":"LGk0D6-v5pJ4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Set up environment"],"metadata":{"id":"u58Vh6QwY59p"}},{"cell_type":"code","source":["!git clone https://github.com/AntixK/PyTorch-VAE"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Aw-NekzAFdiY","executionInfo":{"status":"ok","timestamp":1680480575965,"user_tz":240,"elapsed":12,"user":{"displayName":"Jackson Cornell","userId":"11282324665534091904"}},"outputId":"7961afed-22ed-4b63-8328-1c9288998fe5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'PyTorch-VAE' already exists and is not an empty directory.\n"]}]},{"cell_type":"code","source":["!cd PyTorch-VAE"],"metadata":{"id":"ybmsakk4GJXk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -r PyTorch-VAE/requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aYXOA88kGNdA","executionInfo":{"status":"ok","timestamp":1680480579837,"user_tz":240,"elapsed":3879,"user":{"displayName":"Jackson Cornell","userId":"11282324665534091904"}},"outputId":"e722731c-25ea-4684-d2d2-7c53718d6e16"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pytorch-lightning==1.5.6 in /usr/local/lib/python3.9/dist-packages (from -r PyTorch-VAE/requirements.txt (line 1)) (1.5.6)\n","Requirement already satisfied: PyYAML==6.0 in /usr/local/lib/python3.9/dist-packages (from -r PyTorch-VAE/requirements.txt (line 2)) (6.0)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from -r PyTorch-VAE/requirements.txt (line 3)) (2.12.0)\n","Requirement already satisfied: torch>=1.6.1 in /usr/local/lib/python3.9/dist-packages (from -r PyTorch-VAE/requirements.txt (line 4)) (1.13.1+cu116)\n","Requirement already satisfied: torchsummary==1.5.1 in /usr/local/lib/python3.9/dist-packages (from -r PyTorch-VAE/requirements.txt (line 5)) (1.5.1)\n","Requirement already satisfied: torchvision>=0.10.1 in /usr/local/lib/python3.9/dist-packages (from -r PyTorch-VAE/requirements.txt (line 6)) (0.14.1+cu116)\n","Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.5.6->-r PyTorch-VAE/requirements.txt (line 1)) (2023.3.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.5.6->-r PyTorch-VAE/requirements.txt (line 1)) (4.5.0)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.5.6->-r PyTorch-VAE/requirements.txt (line 1)) (1.22.4)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.5.6->-r PyTorch-VAE/requirements.txt (line 1)) (23.0)\n","Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.5.6->-r PyTorch-VAE/requirements.txt (line 1)) (0.18.3)\n","Requirement already satisfied: torchmetrics>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.5.6->-r PyTorch-VAE/requirements.txt (line 1)) (0.11.4)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.5.6->-r PyTorch-VAE/requirements.txt (line 1)) (4.65.0)\n","Requirement already satisfied: pyDeprecate==0.3.1 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.5.6->-r PyTorch-VAE/requirements.txt (line 1)) (0.3.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (67.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (1.8.1)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (1.53.0)\n","Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (3.20.3)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (0.4.6)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (3.4.3)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (1.4.0)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (2.2.3)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (0.40.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (0.7.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (2.17.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (2.27.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision>=0.10.1->-r PyTorch-VAE/requirements.txt (line 6)) (8.4.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.9/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.6->-r PyTorch-VAE/requirements.txt (line 1)) (3.8.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (0.2.8)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (1.16.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (5.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (6.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (2.0.12)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (2.1.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.6->-r PyTorch-VAE/requirements.txt (line 1)) (1.8.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.6->-r PyTorch-VAE/requirements.txt (line 1)) (22.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.6->-r PyTorch-VAE/requirements.txt (line 1)) (6.0.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.6->-r PyTorch-VAE/requirements.txt (line 1)) (1.3.1)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.6->-r PyTorch-VAE/requirements.txt (line 1)) (1.3.3)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.6->-r PyTorch-VAE/requirements.txt (line 1)) (4.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (3.15.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->-r PyTorch-VAE/requirements.txt (line 3)) (3.2.2)\n"]}]},{"cell_type":"code","source":["import sys\n","sys.path.insert(0, '/content/PyTorch-VAE')"],"metadata":{"id":"CVIws9cbHPkq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import numpy as np\n","from pathlib import Path\n","from PIL import Image\n","from tqdm import tqdm\n","import matplotlib\n","from matplotlib import pyplot as plt\n","\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n","from torchvision import transforms\n","from scipy.stats import pearsonr as corr\n","from torchsummary import summary\n","\n","from models import VanillaVAE"],"metadata":{"id":"PuuLUMZh1Zf_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if platform == 'colab':\n","    from google.colab import drive\n","    drive.mount('/content/drive/', force_remount=True)\n","    data_dir = '/content/drive/MyDrive/algonauts_2023_tutorial_data' #@param {type:\"string\"}\n","    parent_submission_dir = '/content/drive/MyDrive/algonauts_2023_challenge_submission' #@param {type:\"string\"}\n","elif platform == 'jupyter_notebook':\n","    data_dir = '../algonauts_2023_challenge_data'\n","    parent_submission_dir = '../algonauts_2023_challenge_submission'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E1j8NTSDOzEU","executionInfo":{"status":"ok","timestamp":1680480585881,"user_tz":240,"elapsed":5497,"user":{"displayName":"Jackson Cornell","userId":"11282324665534091904"}},"outputId":"119a2b21-8e8d-475b-ddc7-11a200599200"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","source":["device = torch.device(device)"],"metadata":{"id":"XOYmh5NqP-FW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class argObj:\n","  def __init__(self, data_dir, parent_submission_dir, subj):\n","    \n","    self.subj = format(subj, '02')\n","    self.data_dir = os.path.join(data_dir, 'subj'+self.subj)\n","    self.parent_submission_dir = parent_submission_dir\n","    self.subject_submission_dir = os.path.join(self.parent_submission_dir,\n","        'subj'+self.subj)\n","\n","    # Create the submission directory if not existing\n","    if not os.path.isdir(self.subject_submission_dir):\n","        os.makedirs(self.subject_submission_dir)\n","\n","args = argObj(data_dir, parent_submission_dir, subj)"],"metadata":{"id":"XwHWzqmc1M1M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Preprocess data"],"metadata":{"id":"IlFj3_EW0ZZp"}},{"cell_type":"markdown","source":["### Load voxel data"],"metadata":{"id":"xgI2baY6LcwX"}},{"cell_type":"code","source":["fmri_dir = os.path.join(args.data_dir, 'training_split', 'training_fmri')\n","lh_fmri = np.load(os.path.join(fmri_dir, 'lh_training_fmri.npy'))\n","rh_fmri = np.load(os.path.join(fmri_dir, 'rh_training_fmri.npy'))\n","\n","print('LH training fMRI data shape:')\n","print(lh_fmri.shape)\n","print('(Training stimulus images × LH vertices)')\n","\n","print('\\nRH training fMRI data shape:')\n","print(rh_fmri.shape)\n","print('(Training stimulus images × RH vertices)')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"biFldRj3LhE8","executionInfo":{"status":"ok","timestamp":1680480613155,"user_tz":240,"elapsed":27279,"user":{"displayName":"Jackson Cornell","userId":"11282324665534091904"}},"outputId":"919eef8f-63fa-48f8-f64d-e03ee9df7643"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["LH training fMRI data shape:\n","(9082, 18978)\n","(Training stimulus images × LH vertices)\n","\n","RH training fMRI data shape:\n","(9082, 20220)\n","(Training stimulus images × RH vertices)\n"]}]},{"cell_type":"markdown","source":["### Load images"],"metadata":{"id":"Ozloaer713dX"}},{"cell_type":"code","source":["train_img_dir  = os.path.join(args.data_dir, 'training_split', 'training_images')\n","test_img_dir  = os.path.join(args.data_dir, 'test_split', 'test_images')\n","\n","# Create lists will all training and test image file names, sorted\n","train_img_list = os.listdir(train_img_dir)\n","train_img_list.sort()\n","test_img_list = os.listdir(test_img_dir)\n","test_img_list.sort()\n","print('Training images: ' + str(len(train_img_list)))\n","print('Test images: ' + str(len(test_img_list)))"],"metadata":{"id":"1m5C9nkl1rhp","executionInfo":{"status":"ok","timestamp":1680480650355,"user_tz":240,"elapsed":37229,"user":{"displayName":"Jackson Cornell","userId":"11282324665534091904"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a02f8672-bdd5-44bc-f3d0-2c1c652796b3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training images: 9082\n","Test images: 293\n"]}]},{"cell_type":"code","source":["train_img_file = train_img_list[0]\n","print('Training image file name: ' + train_img_file)\n","print('73k NSD images ID: ' + train_img_file[-9:-4])"],"metadata":{"id":"-cAWKqIX12M6","executionInfo":{"status":"ok","timestamp":1680480650358,"user_tz":240,"elapsed":46,"user":{"displayName":"Jackson Cornell","userId":"11282324665534091904"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"41fb1ae0-3551-4955-d55f-af56b6809094"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training image file name: train-0001_nsd-00004.png\n","73k NSD images ID: 00004\n"]}]},{"cell_type":"markdown","source":["### Spit into train/test"],"metadata":{"id":"U9e1e7rR2b8j"}},{"cell_type":"code","source":["np.random.seed(rand_seed)\n","\n","# Calculate how many stimulus images correspond to 90% of the training data\n","num_train = int(np.round(len(train_img_list) / 100 * 90))\n","# Shuffle all training stimulus images\n","idxs = np.arange(len(train_img_list))\n","np.random.shuffle(idxs)\n","# Assign 90% of the shuffled stimulus images to the training partition,\n","# and 10% to the test partition\n","idxs_train, idxs_val = idxs[:num_train], idxs[num_train:]\n","# No need to shuffle or split the test stimulus images\n","idxs_test = np.arange(len(test_img_list))\n","\n","print('Training stimulus images: ' + format(len(idxs_train)))\n","print('\\nValidation stimulus images: ' + format(len(idxs_val)))\n","print('\\nTest stimulus images: ' + format(len(idxs_test)))"],"metadata":{"id":"XUKSQMLq2bNs","executionInfo":{"status":"ok","timestamp":1680480650359,"user_tz":240,"elapsed":41,"user":{"displayName":"Jackson Cornell","userId":"11282324665534091904"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5b40b745-4062-41e6-b7dd-2fc68f8f942d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training stimulus images: 8174\n","\n","Validation stimulus images: 908\n","\n","Test stimulus images: 293\n"]}]},{"cell_type":"markdown","source":["### Create data pipeline"],"metadata":{"id":"skVNGKEB3iIC"}},{"cell_type":"code","source":["transform = transforms.Compose([\n","    transforms.Resize((224, 224)), # resize the images to 224x224 pixels\n","    transforms.ToTensor(), # convert the images to a PyTorch tensor\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), # normalize the images color channels\n","    transforms.RandomRotation(90)\n","    #transforms.ColorJitter(0.5, 0.5, 0.5, 0.5)\n","])"],"metadata":{"id":"hnIRSE2b3IwB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Define class for easy image manipulation"],"metadata":{"id":"zA3VLc8i3qwi"}},{"cell_type":"code","source":["class ImageDataset(Dataset):\n","    def __init__(self, imgs_paths, idxs, transform):\n","        self.imgs_paths = np.array(imgs_paths)[idxs]\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.imgs_paths)\n","\n","    def __getitem__(self, idx):\n","        # Load the image\n","        img_path = self.imgs_paths[idx]\n","        img = Image.open(img_path).convert('RGB')\n","        # Preprocess the image and send it to the chosen device ('cpu' or 'cuda')\n","        if self.transform:\n","            img = self.transform(img).to(device)\n","        return img"],"metadata":{"id":"QOiO6McC3wzq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the paths of all image files\n","train_imgs_paths = sorted(list(Path(train_img_dir).iterdir()))\n","test_imgs_paths = sorted(list(Path(test_img_dir).iterdir()))\n","\n","# The DataLoaders contain the ImageDataset class\n","train_imgs_dataloader = DataLoader(\n","    ImageDataset(train_imgs_paths, idxs_train, transform), \n","    batch_size=batch_size\n",")\n","val_imgs_dataloader = DataLoader(\n","    ImageDataset(train_imgs_paths, idxs_val, transform), \n","    batch_size=batch_size\n",")\n","test_imgs_dataloader = DataLoader(\n","    ImageDataset(test_imgs_paths, idxs_test, transform), \n","    batch_size=batch_size\n",")"],"metadata":{"id":"K9fKJI7YKNaA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Delete fmri data (not needed for training)"],"metadata":{"id":"KPWEwh3qMR5i"}},{"cell_type":"code","source":["del lh_fmri, rh_fmri"],"metadata":{"id":"lfZ_l-iMMSDZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model training"],"metadata":{"id":"KtZJptfUMblK"}},{"cell_type":"markdown","source":["### Load model"],"metadata":{"id":"dbSQkVU3Kw-y"}},{"cell_type":"code","source":["if (retrain):\n","  from google.colab import drive\n","  drive.mount('/content/drive/', force_remount=True)\n","  model = torch.load(os.path.join('/content/drive/MyDrive/', model_path))\n","else:\n","  model = VanillaVAE(in_channels=3, latent_dim=latent_dim)\n","\n","model.to(device)\n","\n","summary(model, (3, 64, 64), device='cuda')"],"metadata":{"id":"KRPfOLNcDQNK","executionInfo":{"status":"ok","timestamp":1680480658685,"user_tz":240,"elapsed":8361,"user":{"displayName":"Jackson Cornell","userId":"11282324665534091904"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9659d2d0-7dfa-45e0-f396-bbace277003a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 32, 32, 32]             896\n","       BatchNorm2d-2           [-1, 32, 32, 32]              64\n","         LeakyReLU-3           [-1, 32, 32, 32]               0\n","            Conv2d-4           [-1, 64, 16, 16]          18,496\n","       BatchNorm2d-5           [-1, 64, 16, 16]             128\n","         LeakyReLU-6           [-1, 64, 16, 16]               0\n","            Conv2d-7            [-1, 128, 8, 8]          73,856\n","       BatchNorm2d-8            [-1, 128, 8, 8]             256\n","         LeakyReLU-9            [-1, 128, 8, 8]               0\n","           Conv2d-10            [-1, 256, 4, 4]         295,168\n","      BatchNorm2d-11            [-1, 256, 4, 4]             512\n","        LeakyReLU-12            [-1, 256, 4, 4]               0\n","           Conv2d-13            [-1, 512, 2, 2]       1,180,160\n","      BatchNorm2d-14            [-1, 512, 2, 2]           1,024\n","        LeakyReLU-15            [-1, 512, 2, 2]               0\n","           Linear-16                  [-1, 100]         204,900\n","           Linear-17                  [-1, 100]         204,900\n","           Linear-18                 [-1, 2048]         206,848\n","  ConvTranspose2d-19            [-1, 256, 4, 4]       1,179,904\n","      BatchNorm2d-20            [-1, 256, 4, 4]             512\n","        LeakyReLU-21            [-1, 256, 4, 4]               0\n","  ConvTranspose2d-22            [-1, 128, 8, 8]         295,040\n","      BatchNorm2d-23            [-1, 128, 8, 8]             256\n","        LeakyReLU-24            [-1, 128, 8, 8]               0\n","  ConvTranspose2d-25           [-1, 64, 16, 16]          73,792\n","      BatchNorm2d-26           [-1, 64, 16, 16]             128\n","        LeakyReLU-27           [-1, 64, 16, 16]               0\n","  ConvTranspose2d-28           [-1, 32, 32, 32]          18,464\n","      BatchNorm2d-29           [-1, 32, 32, 32]              64\n","        LeakyReLU-30           [-1, 32, 32, 32]               0\n","  ConvTranspose2d-31           [-1, 32, 64, 64]           9,248\n","      BatchNorm2d-32           [-1, 32, 64, 64]              64\n","        LeakyReLU-33           [-1, 32, 64, 64]               0\n","           Conv2d-34            [-1, 3, 64, 64]             867\n","             Tanh-35            [-1, 3, 64, 64]               0\n","================================================================\n","Total params: 3,765,547\n","Trainable params: 3,765,547\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.05\n","Forward/backward pass size (MB): 6.06\n","Params size (MB): 14.36\n","Estimated Total Size (MB): 20.48\n","----------------------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["### Define training loop"],"metadata":{"id":"tyJYbxe2K0Nk"}},{"cell_type":"code","source":["def Training(model, dataloader, epochs=1):\n","  \n","  # parameters\n","  lr = 0.005\n","  train_loss = 0.0\n","\n","  # optimizer\n","  optimizer = torch.optim.Adam(model.parameters(),\n","                               lr=lr,\n","                               weight_decay=1e-5)\n","  \n","  # iterate the dataloader\n","  for _, x in tqdm(enumerate(dataloader), total=len(dataloader)):\n","    for _ in range(epochs):\n","\n","      x = x.to(device)\n","\n","      # run data through model\n","      downsampled = torch.nn.functional.interpolate(x, size=[64, 64], mode='bilinear')\n","      out, input, mu, log_var = model(downsampled)\n","      x_hat = torch.nn.functional.interpolate(out, size=[224, 224], mode='bilinear')\n","\n","      # evaluate loss\n","      loss = model.loss_function(out, input, mu, log_var, M_N=1)\n","\n","      # backward pass\n","      optimizer.zero_grad()\n","      loss['loss'].backward()\n","      optimizer.step()\n","\n","      # print batch loss\n","      print('\\t train loss: %f' % (loss['loss'].item()))\n","      train_loss += loss['loss'].item()\n","\n","  return train_loss / len(dataloader.dataset)"],"metadata":{"id":"nOisp0RtQkW7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def Validate(model, dataloader, epochs=1, retrain=False):\n","\n","  # parameters\n","  lr = 0.005\n","  val_loss = 0.0\n","\n","  # optimizer\n","  optimizer = torch.optim.Adam(model.parameters(),\n","                               lr=lr,\n","                               weight_decay=1e-5)\n","  \n","  # iterate the dataloader\n","  for _, x in tqdm(enumerate(dataloader), total=len(dataloader)):\n","    for _ in range(epochs):\n","\n","      x = x.to(device)\n","\n","      # run data through model\n","      downsampled = torch.nn.functional.interpolate(x, size=[64, 64], mode='bilinear')\n","      out, input, mu, log_var = model(downsampled)\n","      x_hat = torch.nn.functional.interpolate(out, size=[224, 224], mode='bilinear')\n","\n","      # evaluate loss\n","      loss = model.loss_function(out, input, mu, log_var, M_N=1)\n","\n","      # backward pass\n","      if (retrain):\n","        optimizer.zero_grad()\n","        loss['loss'].backward()\n","        optimizer.step()\n","\n","      # print batch loss\n","      print('\\t validation loss: %f' % (loss['loss'].item()))\n","      val_loss += loss['loss'].item()\n","\n","  return val_loss / len(dataloader.dataset)"],"metadata":{"id":"-DdiohbkK-9P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Train variational autoencoder"],"metadata":{"id":"4RvPdlt4K2mb"}},{"cell_type":"code","source":["train_loss = Training(model, train_imgs_dataloader)"],"metadata":{"id":"H23MJeTiQk1a","executionInfo":{"status":"error","timestamp":1680482163249,"user_tz":240,"elapsed":1504597,"user":{"displayName":"Jackson Cornell","userId":"11282324665534091904"}},"colab":{"base_uri":"https://localhost:8080/","height":780},"outputId":"194eb9a1-28a5-48e7-cef8-c93408cf1696"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["  1%|          | 1/128 [00:58<2:04:50, 58.98s/it]"]},{"output_type":"stream","name":"stdout","text":["\t train loss: 1.251184\n"]},{"output_type":"stream","name":"stderr","text":["\r  2%|▏         | 2/128 [01:57<2:03:11, 58.66s/it]"]},{"output_type":"stream","name":"stdout","text":["\t train loss: 1.343740\n"]},{"output_type":"stream","name":"stderr","text":["\r  2%|▏         | 3/128 [02:54<2:00:36, 57.89s/it]"]},{"output_type":"stream","name":"stdout","text":["\t train loss: 1.259307\n"]},{"output_type":"stream","name":"stderr","text":["\r  3%|▎         | 4/128 [03:51<1:59:14, 57.70s/it]"]},{"output_type":"stream","name":"stdout","text":["\t train loss: 1.285219\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 5/128 [04:49<1:58:38, 57.88s/it]"]},{"output_type":"stream","name":"stdout","text":["\t train loss: 1.316059\n"]},{"output_type":"stream","name":"stderr","text":["\r  5%|▍         | 6/128 [05:48<1:57:50, 57.95s/it]"]},{"output_type":"stream","name":"stdout","text":["\t train loss: 1.261640\n"]},{"output_type":"stream","name":"stderr","text":["\r  5%|▌         | 7/128 [06:44<1:55:58, 57.51s/it]"]},{"output_type":"stream","name":"stdout","text":["\t train loss: 1.264335\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▋         | 8/128 [07:42<1:55:09, 57.58s/it]"]},{"output_type":"stream","name":"stdout","text":["\t train loss: 1.340415\n"]},{"output_type":"stream","name":"stderr","text":["\r  7%|▋         | 9/128 [08:40<1:54:27, 57.71s/it]"]},{"output_type":"stream","name":"stdout","text":["\t train loss: 1.269695\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 10/128 [09:38<1:53:33, 57.74s/it]"]},{"output_type":"stream","name":"stdout","text":["\t train loss: 1.193209\n"]},{"output_type":"stream","name":"stderr","text":["\r  9%|▊         | 11/128 [10:35<1:52:19, 57.60s/it]"]},{"output_type":"stream","name":"stdout","text":["\t train loss: 1.418529\n"]},{"output_type":"stream","name":"stderr","text":["\r  9%|▉         | 12/128 [11:32<1:50:53, 57.36s/it]"]},{"output_type":"stream","name":"stdout","text":["\t train loss: 1.258581\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 13/128 [12:28<1:49:23, 57.07s/it]"]},{"output_type":"stream","name":"stdout","text":["\t train loss: 1.184211\n"]},{"output_type":"stream","name":"stderr","text":["\r 11%|█         | 14/128 [13:25<1:48:13, 56.96s/it]"]},{"output_type":"stream","name":"stdout","text":["\t train loss: 1.215200\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 15/128 [14:22<1:47:29, 57.07s/it]"]},{"output_type":"stream","name":"stdout","text":["\t train loss: 1.233599\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▎        | 16/128 [15:20<1:47:06, 57.38s/it]"]},{"output_type":"stream","name":"stdout","text":["\t train loss: 1.238379\n"]},{"output_type":"stream","name":"stderr","text":["\r 13%|█▎        | 17/128 [16:17<1:45:55, 57.26s/it]"]},{"output_type":"stream","name":"stdout","text":["\t train loss: 1.291192\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 18/128 [17:14<1:44:29, 57.00s/it]"]},{"output_type":"stream","name":"stdout","text":["\t train loss: 1.230030\n"]},{"output_type":"stream","name":"stderr","text":["\r 15%|█▍        | 19/128 [18:10<1:43:08, 56.77s/it]"]},{"output_type":"stream","name":"stdout","text":["\t train loss: 1.256905\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 20/128 [19:06<1:41:58, 56.65s/it]"]},{"output_type":"stream","name":"stdout","text":["\t train loss: 1.224156\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▋        | 21/128 [20:03<1:41:03, 56.67s/it]"]},{"output_type":"stream","name":"stdout","text":["\t train loss: 1.157879\n"]},{"output_type":"stream","name":"stderr","text":["\r 17%|█▋        | 22/128 [21:01<1:41:03, 57.20s/it]"]},{"output_type":"stream","name":"stdout","text":["\t train loss: 1.192342\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 23/128 [21:58<1:39:36, 56.92s/it]"]},{"output_type":"stream","name":"stdout","text":["\t train loss: 1.231669\n"]},{"output_type":"stream","name":"stderr","text":["\r 19%|█▉        | 24/128 [22:54<1:38:31, 56.84s/it]"]},{"output_type":"stream","name":"stdout","text":["\t train loss: 1.230034\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|█▉        | 25/128 [23:51<1:37:14, 56.65s/it]"]},{"output_type":"stream","name":"stdout","text":["\t train loss: 1.124402\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 26/128 [24:47<1:36:20, 56.67s/it]"]},{"output_type":"stream","name":"stdout","text":["\t train loss: 1.385329\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 26/128 [25:04<1:38:22, 57.86s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-9e80b42cebe4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_imgs_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-19-0753116bd8c4>\u001b[0m in \u001b[0;36mTraining\u001b[0;34m(model, dataloader, epochs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;31m# iterate the dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-5a1da0afda89>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Load the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Preprocess the image and send it to the chosen device ('cpu' or 'cuda')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2982\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2984\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2986\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["val_loss = Training(model, val_imgs_dataloader, epochs=2)"],"metadata":{"id":"KXR2kfXBnzSM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_loss = Validate(model, test_imgs_dataloader, epochs=2, retrain=True)"],"metadata":{"id":"e5vQ9zoMsg_M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Save VAE"],"metadata":{"id":"w8ZAh3fvswIH"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)\n","\n","torch.save(model, os.path.join('/content/drive/MyDrive/', model_path))"],"metadata":{"id":"U3uFAF0KszY_","executionInfo":{"status":"ok","timestamp":1680482176583,"user_tz":240,"elapsed":7268,"user":{"displayName":"Jackson Cornell","userId":"11282324665534091904"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2feae2ee-6bd5-4a33-da7b-f8dff82e1dbf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]}]}