{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.applications.xception import preprocess_input\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tqdm import tqdm\n",
        "from scipy.stats import pearsonr as corr\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "data_dir = '/content/drive/MyDrive/ColabNotebooks/NeuroAI:FINALPROJECT/algonauts_2023_tutorial_data'\n",
        "parent_submission_dir = '/content/drive/MyDrive/ColabNotebooks/NeuroAI:FINALPROJECT/algonauts_2023_challenge_submission'"
      ],
      "metadata": {
        "id": "oblDW0fdw4Ow",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbc3ceb8-6552-404d-aa2d-fa2ae69919a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train/Validation Block\n",
        "It is likely memory will crash if you do not specify a smaller list of ```subj```."
      ],
      "metadata": {
        "id": "povAkrUVvQxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for subj in [1, 2, 3, 4, 5, 6, 7, 8]: \n",
        "  print('---------- Subject '+str(subj)+' ----------')\n",
        "  class argObj:\n",
        "    def __init__(self, data_dir, parent_submission_dir, subj):\n",
        "      \n",
        "      self.subj = format(subj, '02')\n",
        "      self.data_dir = os.path.join(data_dir, 'subj'+self.subj)\n",
        "      self.parent_submission_dir = parent_submission_dir\n",
        "      self.subject_submission_dir = os.path.join(self.parent_submission_dir,\n",
        "          'subj'+self.subj)\n",
        "\n",
        "      # Create the submission directory if not existing\n",
        "      if not os.path.isdir(self.subject_submission_dir):\n",
        "          os.makedirs(self.subject_submission_dir)\n",
        "\n",
        "  args = argObj(data_dir, parent_submission_dir, subj)\n",
        "\n",
        "  # ----- LOAD fMRI DATA -----\n",
        "  fmri_dir = os.path.join(args.data_dir, 'training_split', 'training_fmri')\n",
        "  lh_fmri = np.load(os.path.join(fmri_dir, 'lh_training_fmri.npy'))\n",
        "  rh_fmri = np.load(os.path.join(fmri_dir, 'rh_training_fmri.npy'))\n",
        "\n",
        "  print('LH training fMRI data shape:')\n",
        "  print(lh_fmri.shape)\n",
        "  print('(Training stimulus images × LH vertices)')\n",
        "\n",
        "  print('\\nRH training fMRI data shape:')\n",
        "  print(rh_fmri.shape)\n",
        "  print('(Training stimulus images × RH vertices)')\n",
        "\n",
        "  # ----- LOAD IMAGE LOCATION -----\n",
        "  train_img_dir  = os.path.join(args.data_dir, 'training_split', 'training_images')\n",
        "\n",
        "  # ----- LOAD IMAGE DATA & PREPROCESS -----\n",
        "  # Read in, store, and preprocess training images\n",
        "  train_imgs = []\n",
        "  for path in sorted(list(Path(train_img_dir).iterdir())):\n",
        "    img = load_img(path, target_size=(299, 299))\n",
        "    img_array = img_to_array(img)\n",
        "    img_processed = preprocess_input(img_array)\n",
        "    train_imgs.append(img_processed)\n",
        "  train_imgs = np.array(train_imgs)\n",
        "\n",
        "  # ----- SPLIT DATA -----\n",
        "  imgs_train, imgs_val, lh_fmri_train, lh_fmri_val, rh_fmri_train, rh_fmri_val = train_test_split(train_imgs, \n",
        "                                                                                                  lh_fmri,\n",
        "                                                                                                  rh_fmri,\n",
        "                                                                                                  test_size=0.2, \n",
        "                                                                                                  random_state=0)\n",
        "  print('\\nImage Shapes:')\n",
        "  print(imgs_train.shape, imgs_val.shape)    \n",
        "  print('\\nLeft fMRI Voxel Data Shapes:')\n",
        "  print(lh_fmri_train.shape, lh_fmri_val.shape)   \n",
        "  print('Right fMRI Voxel Data Shapes:\\n')\n",
        "  print(rh_fmri_train.shape, rh_fmri_val.shape)\n",
        "\n",
        "  # Make model for left and right hemisphere \n",
        "  # 0 = Left, 1 = Right\n",
        "  for hemi in ['L', 'R']:\n",
        "    # ----- DEFINE MODEL -----\n",
        "    data_augmentation = keras.Sequential(\n",
        "        [keras.layers.RandomFlip(\"horizontal\"), keras.layers.RandomRotation(0.1),]\n",
        "    )\n",
        "\n",
        "    # Create model\n",
        "    # Load the ResNet50 model with pre-trained weights\n",
        "    base_model = keras.applications.Xception(include_top=False)\n",
        "\n",
        "    # Freeze the convolutional layers of the ResNet50 model\n",
        "    base_model.trainable = False\n",
        "\n",
        "    inputs = keras.Input(shape=(299, 299, 3))\n",
        "    x = data_augmentation(inputs) # augment data to generalize\n",
        "    x = base_model(x, training=False) # start with resnet50\n",
        "    x = keras.layers.GlobalAveragePooling2D()(x) # pool\n",
        "    x = keras.layers.Dropout(0.2)(x)  # regularize with dropout\n",
        "\n",
        "    # Choose correct hemisphere size\n",
        "    num_voxels = lh_fmri_train.shape[1]\n",
        "    if hemi == 'R':\n",
        "      num_voxels = rh_fmri_train.shape[1]\n",
        "      \n",
        "    outputs = keras.layers.Dense(num_voxels, activation='linear', kernel_regularizer=keras.regularizers.l2(0.01))(x) # define voxels to predict\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    # Compile the encoding model\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Train the top layer\n",
        "    if hemi == 'L':\n",
        "      model.fit(imgs_train, lh_fmri_train, validation_data=(imgs_val, lh_fmri_val), epochs=20, batch_size=32, verbose=1)\n",
        "    else:\n",
        "      model.fit(imgs_train, rh_fmri_train, validation_data=(imgs_val, rh_fmri_val), epochs=20, batch_size=32, verbose=1)\n",
        "    print()\n",
        "    # Unfreeze the last few convolutional layers for fine-tuning\n",
        "    base_model.trainable = True\n",
        "\n",
        "    # Lower the learning rate for fine-tuning\n",
        "    optimizer = Adam(learning_rate=1e-5)\n",
        "\n",
        "    # Recompile the model with the new optimizer\n",
        "    model.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "    # Train the model on your own dataset\n",
        "    if hemi == 'L':\n",
        "      model.fit(imgs_train, lh_fmri_train, validation_data=(imgs_val, lh_fmri_val), epochs=10, batch_size=32, verbose=1)\n",
        "    else:\n",
        "      model.fit(imgs_train, rh_fmri_train, validation_data=(imgs_val, rh_fmri_val), epochs=10, batch_size=32, verbose=1)\n",
        "    print()\n",
        "    # Validation Prediction\n",
        "    fmri_val_pred = model.predict(imgs_val)\n",
        "\n",
        "    # Get corrleation of predictions\n",
        "    correlation = np.zeros(fmri_val_pred.shape[1])\n",
        "    for v in tqdm(range(fmri_val_pred.shape[1])):\n",
        "      if hemi == 'L':\n",
        "        correlation[v] = corr(fmri_val_pred[:,v], lh_fmri_val[:,v])[0]\n",
        "      else:\n",
        "        correlation[v] = corr(fmri_val_pred[:,v], rh_fmri_val[:,v])[0]\n",
        "    print('\\nMean Corr Val: Subj ' +str(subj)+' '+hemi)\n",
        "    print(np.mean(correlation))\n",
        "    print()\n",
        "\n",
        "    # Save Model\n",
        "    model.save('/content/drive/MyDrive/ColabNotebooks/NeuroAI:FINALPROJECT/model'+str(subj)+hemi+'.h5')"
      ],
      "metadata": {
        "id": "_gtkO-ThRQvY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8632bebe-91e5-407d-f2f6-2332987e6950"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- Subject 7 ----------\n",
            "LH training fMRI data shape:\n",
            "(9841, 19004)\n",
            "(Training stimulus images × LH vertices)\n",
            "\n",
            "RH training fMRI data shape:\n",
            "(9841, 20544)\n",
            "(Training stimulus images × RH vertices)\n",
            "\n",
            "Image Shapes:\n",
            "(7872, 299, 299, 3) (1969, 299, 299, 3)\n",
            "\n",
            "Left fMRI Voxel Data Shapes:\n",
            "(7872, 19004) (1969, 19004)\n",
            "Right fMRI Voxel Data Shapes:\n",
            "\n",
            "(7872, 20544) (1969, 20544)\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83683744/83683744 [==============================] - 4s 0us/step\n",
            "Epoch 1/20\n",
            "246/246 [==============================] - 17s 52ms/step - loss: 1.3032 - val_loss: 0.4697\n",
            "Epoch 2/20\n",
            "246/246 [==============================] - 11s 43ms/step - loss: 0.4701 - val_loss: 0.4715\n",
            "Epoch 3/20\n",
            "246/246 [==============================] - 10s 43ms/step - loss: 0.4717 - val_loss: 0.4721\n",
            "Epoch 4/20\n",
            "246/246 [==============================] - 10s 43ms/step - loss: 0.4729 - val_loss: 0.4744\n",
            "Epoch 5/20\n",
            "246/246 [==============================] - 10s 43ms/step - loss: 0.4740 - val_loss: 0.4747\n",
            "Epoch 6/20\n",
            "246/246 [==============================] - 10s 42ms/step - loss: 0.4748 - val_loss: 0.4765\n",
            "Epoch 7/20\n",
            "246/246 [==============================] - 11s 43ms/step - loss: 0.4759 - val_loss: 0.4767\n",
            "Epoch 8/20\n",
            "246/246 [==============================] - 10s 42ms/step - loss: 0.4768 - val_loss: 0.4772\n",
            "Epoch 9/20\n",
            "246/246 [==============================] - 11s 43ms/step - loss: 0.4775 - val_loss: 0.4793\n",
            "Epoch 10/20\n",
            "246/246 [==============================] - 10s 43ms/step - loss: 0.4777 - val_loss: 0.4776\n",
            "Epoch 11/20\n",
            "246/246 [==============================] - 11s 43ms/step - loss: 0.4787 - val_loss: 0.4791\n",
            "Epoch 12/20\n",
            "246/246 [==============================] - 11s 43ms/step - loss: 0.4793 - val_loss: 0.4795\n",
            "Epoch 13/20\n",
            "246/246 [==============================] - 11s 43ms/step - loss: 0.4793 - val_loss: 0.4809\n",
            "Epoch 14/20\n",
            "246/246 [==============================] - 10s 43ms/step - loss: 0.4801 - val_loss: 0.4810\n",
            "Epoch 15/20\n",
            "246/246 [==============================] - 11s 43ms/step - loss: 0.4802 - val_loss: 0.4805\n",
            "Epoch 16/20\n",
            "246/246 [==============================] - 11s 43ms/step - loss: 0.4803 - val_loss: 0.4795\n",
            "Epoch 17/20\n",
            "246/246 [==============================] - 11s 43ms/step - loss: 0.4806 - val_loss: 0.4801\n",
            "Epoch 18/20\n",
            "246/246 [==============================] - 11s 43ms/step - loss: 0.4807 - val_loss: 0.4836\n",
            "Epoch 19/20\n",
            "246/246 [==============================] - 11s 43ms/step - loss: 0.4806 - val_loss: 0.4828\n",
            "Epoch 20/20\n",
            "246/246 [==============================] - 10s 43ms/step - loss: 0.4809 - val_loss: 0.4802\n",
            "\n",
            "Epoch 1/10\n",
            "246/246 [==============================] - 65s 145ms/step - loss: 0.4484 - val_loss: 0.4268\n",
            "Epoch 2/10\n",
            "246/246 [==============================] - 34s 137ms/step - loss: 0.4173 - val_loss: 0.4147\n",
            "Epoch 3/10\n",
            "246/246 [==============================] - 34s 137ms/step - loss: 0.4101 - val_loss: 0.4107\n",
            "Epoch 4/10\n",
            "246/246 [==============================] - 34s 137ms/step - loss: 0.4057 - val_loss: 0.4089\n",
            "Epoch 5/10\n",
            "246/246 [==============================] - 34s 136ms/step - loss: 0.4018 - val_loss: 0.4067\n",
            "Epoch 6/10\n",
            "246/246 [==============================] - 34s 137ms/step - loss: 0.3988 - val_loss: 0.4083\n",
            "Epoch 7/10\n",
            "246/246 [==============================] - 34s 137ms/step - loss: 0.3955 - val_loss: 0.4073\n",
            "Epoch 8/10\n",
            "246/246 [==============================] - 34s 137ms/step - loss: 0.3935 - val_loss: 0.4060\n",
            "Epoch 9/10\n",
            "246/246 [==============================] - 34s 137ms/step - loss: 0.3904 - val_loss: 0.4056\n",
            "Epoch 10/10\n",
            "246/246 [==============================] - 34s 137ms/step - loss: 0.3878 - val_loss: 0.4062\n",
            "\n",
            "62/62 [==============================] - 3s 31ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19004/19004 [00:23<00:00, 795.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mean Corr Val: Subj 7 L\n",
            "0.3399097441705209\n",
            "\n",
            "Epoch 1/20\n",
            "246/246 [==============================] - 15s 52ms/step - loss: 1.2874 - val_loss: 0.4670\n",
            "Epoch 2/20\n",
            "246/246 [==============================] - 10s 43ms/step - loss: 0.4669 - val_loss: 0.4691\n",
            "Epoch 3/20\n",
            "246/246 [==============================] - 11s 43ms/step - loss: 0.4684 - val_loss: 0.4702\n",
            "Epoch 4/20\n",
            "246/246 [==============================] - 10s 43ms/step - loss: 0.4695 - val_loss: 0.4717\n",
            "Epoch 5/20\n",
            "246/246 [==============================] - 11s 43ms/step - loss: 0.4708 - val_loss: 0.4738\n",
            "Epoch 6/20\n",
            "246/246 [==============================] - 11s 43ms/step - loss: 0.4716 - val_loss: 0.4734\n",
            "Epoch 7/20\n",
            "246/246 [==============================] - 11s 43ms/step - loss: 0.4724 - val_loss: 0.4767\n",
            "Epoch 8/20\n",
            "246/246 [==============================] - 11s 43ms/step - loss: 0.4734 - val_loss: 0.4734\n",
            "Epoch 9/20\n",
            "246/246 [==============================] - 10s 43ms/step - loss: 0.4739 - val_loss: 0.4757\n",
            "Epoch 10/20\n",
            "246/246 [==============================] - 10s 43ms/step - loss: 0.4750 - val_loss: 0.4765\n",
            "Epoch 11/20\n",
            "246/246 [==============================] - 10s 43ms/step - loss: 0.4752 - val_loss: 0.4759\n",
            "Epoch 12/20\n",
            "246/246 [==============================] - 11s 43ms/step - loss: 0.4762 - val_loss: 0.4800\n",
            "Epoch 13/20\n",
            "246/246 [==============================] - 11s 43ms/step - loss: 0.4760 - val_loss: 0.4777\n",
            "Epoch 14/20\n",
            "246/246 [==============================] - 11s 43ms/step - loss: 0.4768 - val_loss: 0.4763\n",
            "Epoch 15/20\n",
            "246/246 [==============================] - 11s 43ms/step - loss: 0.4769 - val_loss: 0.4777\n",
            "Epoch 16/20\n",
            "246/246 [==============================] - 11s 44ms/step - loss: 0.4771 - val_loss: 0.4793\n",
            "Epoch 17/20\n",
            "246/246 [==============================] - 11s 43ms/step - loss: 0.4772 - val_loss: 0.4806\n",
            "Epoch 18/20\n",
            "246/246 [==============================] - 10s 43ms/step - loss: 0.4779 - val_loss: 0.4770\n",
            "Epoch 19/20\n",
            "246/246 [==============================] - 11s 43ms/step - loss: 0.4776 - val_loss: 0.4781\n",
            "Epoch 20/20\n",
            "246/246 [==============================] - 10s 42ms/step - loss: 0.4771 - val_loss: 0.4795\n",
            "\n",
            "Epoch 1/10\n",
            "246/246 [==============================] - 63s 146ms/step - loss: 0.4423 - val_loss: 0.4223\n",
            "Epoch 2/10\n",
            "246/246 [==============================] - 34s 137ms/step - loss: 0.4127 - val_loss: 0.4102\n",
            "Epoch 3/10\n",
            "246/246 [==============================] - 34s 137ms/step - loss: 0.4061 - val_loss: 0.4073\n",
            "Epoch 4/10\n",
            "246/246 [==============================] - 34s 137ms/step - loss: 0.4017 - val_loss: 0.4061\n",
            "Epoch 5/10\n",
            "246/246 [==============================] - 34s 137ms/step - loss: 0.3983 - val_loss: 0.4058\n",
            "Epoch 6/10\n",
            "246/246 [==============================] - 34s 137ms/step - loss: 0.3957 - val_loss: 0.4037\n",
            "Epoch 7/10\n",
            "246/246 [==============================] - 34s 137ms/step - loss: 0.3925 - val_loss: 0.4028\n",
            "Epoch 8/10\n",
            "246/246 [==============================] - 34s 137ms/step - loss: 0.3902 - val_loss: 0.4030\n",
            "Epoch 9/10\n",
            "246/246 [==============================] - 34s 137ms/step - loss: 0.3875 - val_loss: 0.4029\n",
            "Epoch 10/10\n",
            "246/246 [==============================] - 34s 137ms/step - loss: 0.3855 - val_loss: 0.4033\n",
            "\n",
            "62/62 [==============================] - 3s 30ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20544/20544 [00:26<00:00, 772.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mean Corr Val: Subj 7 R\n",
            "0.3322917732492202\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Model Training Block\n",
        "Run this code when prepping to submit to challenge. There is no validation set. <br/>\n",
        "It is likely memory will crash if you do not specify a smaller list of ```subj```."
      ],
      "metadata": {
        "id": "qd7RZMoyA47c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for subj in [1, 2, 3, 4, 5, 6, 7, 8]: \n",
        "  print('---------- Subject '+str(subj)+' ----------')\n",
        "  class argObj:\n",
        "    def __init__(self, data_dir, parent_submission_dir, subj):\n",
        "      \n",
        "      self.subj = format(subj, '02')\n",
        "      self.data_dir = os.path.join(data_dir, 'subj'+self.subj)\n",
        "      self.parent_submission_dir = parent_submission_dir\n",
        "      self.subject_submission_dir = os.path.join(self.parent_submission_dir,\n",
        "          'subj'+self.subj)\n",
        "\n",
        "      # Create the submission directory if not existing\n",
        "      if not os.path.isdir(self.subject_submission_dir):\n",
        "          os.makedirs(self.subject_submission_dir)\n",
        "\n",
        "  args = argObj(data_dir, parent_submission_dir, subj)\n",
        "\n",
        "  # ----- LOAD fMRI DATA -----\n",
        "  fmri_dir = os.path.join(args.data_dir, 'training_split', 'training_fmri')\n",
        "  lh_fmri = np.load(os.path.join(fmri_dir, 'lh_training_fmri.npy'))\n",
        "  rh_fmri = np.load(os.path.join(fmri_dir, 'rh_training_fmri.npy'))\n",
        "\n",
        "  print('LH training fMRI data shape:')\n",
        "  print(lh_fmri.shape)\n",
        "  print('(Training stimulus images × LH vertices)')\n",
        "\n",
        "  print('\\nRH training fMRI data shape:')\n",
        "  print(rh_fmri.shape)\n",
        "  print('(Training stimulus images × RH vertices)')\n",
        "\n",
        "  # ----- LOAD IMAGE LOCATION -----\n",
        "  train_img_dir  = os.path.join(args.data_dir, 'training_split', 'training_images')\n",
        "\n",
        "  # ----- LOAD IMAGE DATA & PREPROCESS -----\n",
        "  # Read in, store, and preprocess training images\n",
        "  train_imgs = []\n",
        "  for path in sorted(list(Path(train_img_dir).iterdir())):\n",
        "    img = load_img(path, target_size=(299, 299))\n",
        "    img_array = img_to_array(img)\n",
        "    img_processed = preprocess_input(img_array)\n",
        "    train_imgs.append(img_processed)\n",
        "  train_imgs = np.array(train_imgs)\n",
        "\n",
        "  print('\\nImage Shapes:')\n",
        "  print(train_imgs.shape)    \n",
        "  print('\\nLeft fMRI Voxel Data Shapes:')\n",
        "  print(lh_fmri.shape)   \n",
        "  print('Right fMRI Voxel Data Shapes:\\n')\n",
        "  print(rh_fmri.shape)\n",
        "\n",
        "  # Make model for left and right hemisphere \n",
        "  # 0 = Left, 1 = Right\n",
        "  for hemi in ['L', 'R']:\n",
        "    # ----- DEFINE MODEL -----\n",
        "    data_augmentation = keras.Sequential(\n",
        "        [keras.layers.RandomFlip(\"horizontal\"), keras.layers.RandomRotation(0.1),]\n",
        "    )\n",
        "\n",
        "    # Create model\n",
        "    # Load the ResNet50 model with pre-trained weights\n",
        "    base_model = keras.applications.Xception(include_top=False)\n",
        "\n",
        "    # Freeze the convolutional layers of the ResNet50 model\n",
        "    base_model.trainable = False\n",
        "\n",
        "    inputs = keras.Input(shape=(299, 299, 3))\n",
        "    x = data_augmentation(inputs) # augment data to generalize\n",
        "    x = base_model(x, training=False) # start with resnet50\n",
        "    x = keras.layers.GlobalAveragePooling2D()(x) # pool\n",
        "    x = keras.layers.Dropout(0.2)(x)  # regularize with dropout\n",
        "\n",
        "    # Choose correct hemisphere size\n",
        "    num_voxels = lh_fmri.shape[1]\n",
        "    if hemi == 'R':\n",
        "      num_voxels = rh_fmri.shape[1]\n",
        "      \n",
        "    outputs = keras.layers.Dense(num_voxels, activation='linear', kernel_regularizer=keras.regularizers.l2(0.01))(x) # define voxels to predict\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    # Compile the encoding model\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Train the top layer\n",
        "    if hemi == 'L':\n",
        "      model.fit(train_imgs, lh_fmri, epochs=20, batch_size=32, verbose=1)\n",
        "    else:\n",
        "      model.fit(train_imgs, rh_fmri, epochs=20, batch_size=32, verbose=1)\n",
        "\n",
        "    # Unfreeze the last few convolutional layers for fine-tuning\n",
        "    base_model.trainable = True\n",
        "\n",
        "    # Lower the learning rate for fine-tuning\n",
        "    optimizer = Adam(learning_rate=1e-5)\n",
        "\n",
        "    # Recompile the model with the new optimizer\n",
        "    model.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "    # Train the model on your own dataset\n",
        "    if hemi == 'L':\n",
        "      model.fit(train_imgs, lh_fmri, epochs=10, batch_size=32, verbose=1)\n",
        "    else:\n",
        "      model.fit(train_imgs, rh_fmri, epochs=10, batch_size=32, verbose=1)\n",
        "    \n",
        "    # Save Model\n",
        "    model.save('/content/drive/MyDrive/ColabNotebooks/NeuroAI:FINALPROJECT/final_model'+str(subj)+hemi+'.h5')\n"
      ],
      "metadata": {
        "id": "irHK1qQTqZGN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7164f17-dadc-4973-e34f-5c85b32b80d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- Subject 8 ----------\n",
            "LH training fMRI data shape:\n",
            "(8779, 18981)\n",
            "(Training stimulus images × LH vertices)\n",
            "\n",
            "RH training fMRI data shape:\n",
            "(8779, 20530)\n",
            "(Training stimulus images × RH vertices)\n",
            "\n",
            "Image Shapes:\n",
            "(8779, 299, 299, 3)\n",
            "\n",
            "Left fMRI Voxel Data Shapes:\n",
            "(8779, 18981)\n",
            "Right fMRI Voxel Data Shapes:\n",
            "\n",
            "(8779, 20530)\n",
            "Epoch 1/20\n",
            "275/275 [==============================] - 16s 36ms/step - loss: 1.3146\n",
            "Epoch 2/20\n",
            "275/275 [==============================] - 10s 36ms/step - loss: 0.5796\n",
            "Epoch 3/20\n",
            "275/275 [==============================] - 10s 36ms/step - loss: 0.5814\n",
            "Epoch 4/20\n",
            "275/275 [==============================] - 10s 36ms/step - loss: 0.5832\n",
            "Epoch 5/20\n",
            "275/275 [==============================] - 10s 36ms/step - loss: 0.5843\n",
            "Epoch 6/20\n",
            "275/275 [==============================] - 10s 36ms/step - loss: 0.5853\n",
            "Epoch 7/20\n",
            "275/275 [==============================] - 10s 36ms/step - loss: 0.5865\n",
            "Epoch 8/20\n",
            "275/275 [==============================] - 10s 36ms/step - loss: 0.5871\n",
            "Epoch 9/20\n",
            "275/275 [==============================] - 10s 36ms/step - loss: 0.5878\n",
            "Epoch 10/20\n",
            "275/275 [==============================] - 10s 36ms/step - loss: 0.5888\n",
            "Epoch 11/20\n",
            "275/275 [==============================] - 10s 36ms/step - loss: 0.5897\n",
            "Epoch 12/20\n",
            "275/275 [==============================] - 10s 36ms/step - loss: 0.5895\n",
            "Epoch 13/20\n",
            "275/275 [==============================] - 10s 36ms/step - loss: 0.5902\n",
            "Epoch 14/20\n",
            "275/275 [==============================] - 10s 36ms/step - loss: 0.5903\n",
            "Epoch 15/20\n",
            "275/275 [==============================] - 10s 36ms/step - loss: 0.5907\n",
            "Epoch 16/20\n",
            "275/275 [==============================] - 10s 36ms/step - loss: 0.5909\n",
            "Epoch 17/20\n",
            "275/275 [==============================] - 10s 36ms/step - loss: 0.5914\n",
            "Epoch 18/20\n",
            "275/275 [==============================] - 10s 36ms/step - loss: 0.5914\n",
            "Epoch 19/20\n",
            "275/275 [==============================] - 10s 36ms/step - loss: 0.5907\n",
            "Epoch 20/20\n",
            "275/275 [==============================] - 10s 36ms/step - loss: 0.5910\n",
            "Epoch 1/10\n",
            "275/275 [==============================] - 68s 136ms/step - loss: 0.5547\n",
            "Epoch 2/10\n",
            "275/275 [==============================] - 36s 131ms/step - loss: 0.5275\n",
            "Epoch 3/10\n",
            "275/275 [==============================] - 36s 130ms/step - loss: 0.5216\n",
            "Epoch 4/10\n",
            "275/275 [==============================] - 36s 131ms/step - loss: 0.5179\n",
            "Epoch 5/10\n",
            "275/275 [==============================] - 36s 130ms/step - loss: 0.5156\n",
            "Epoch 6/10\n",
            "275/275 [==============================] - 36s 130ms/step - loss: 0.5120\n",
            "Epoch 7/10\n",
            "275/275 [==============================] - 36s 130ms/step - loss: 0.5092\n",
            "Epoch 8/10\n",
            "275/275 [==============================] - 36s 130ms/step - loss: 0.5075\n",
            "Epoch 9/10\n",
            "275/275 [==============================] - 36s 130ms/step - loss: 0.5039\n",
            "Epoch 10/10\n",
            "275/275 [==============================] - 36s 130ms/step - loss: 0.5018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Final Models on Algonauts Test Set"
      ],
      "metadata": {
        "id": "1vWGQSz8Lm5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for subj in [1, 2, 3, 4, 5, 6, 7, 8]: \n",
        "  print('---------- Subject '+str(subj)+' ----------')\n",
        "  class argObj:\n",
        "    def __init__(self, data_dir, parent_submission_dir, subj):\n",
        "      \n",
        "      self.subj = format(subj, '02')\n",
        "      self.data_dir = os.path.join(data_dir, 'subj'+self.subj)\n",
        "      self.parent_submission_dir = parent_submission_dir\n",
        "      self.subject_submission_dir = os.path.join(self.parent_submission_dir,\n",
        "          'subj'+self.subj)\n",
        "\n",
        "      # Create the submission directory if not existing\n",
        "      if not os.path.isdir(self.subject_submission_dir):\n",
        "          os.makedirs(self.subject_submission_dir)\n",
        "\n",
        "  args = argObj(data_dir, parent_submission_dir, subj)\n",
        "\n",
        "  # ----- LOAD IMAGE LOCATION -----\n",
        "  test_img_dir  = os.path.join(args.data_dir, 'test_split', 'test_images')\n",
        "\n",
        "  # ----- LOAD IMAGE DATA & PREPROCESS -----\n",
        "  # Read in, store, and preprocess test images\n",
        "  test_imgs = []\n",
        "  for path in sorted(list(Path(test_img_dir).iterdir())):\n",
        "    img = load_img(path, target_size=(299, 299))\n",
        "    img_array = img_to_array(img)\n",
        "    img_processed = preprocess_input(img_array)\n",
        "    test_imgs.append(img_processed)\n",
        "  test_imgs = np.array(test_imgs)\n",
        "\n",
        "  # ----- RUN MODELS -----\n",
        "  model_L = load_model('/content/drive/MyDrive/ColabNotebooks/NeuroAI:FINALPROJECT/final_model'+str(subj)+'L.h5')\n",
        "  model_R = load_model('/content/drive/MyDrive/ColabNotebooks/NeuroAI:FINALPROJECT/final_model'+str(subj)+'R.h5')\n",
        "  lh_fmri_test_pred = model_L.predict(test_imgs)\n",
        "  rh_fmri_test_pred = model_R.predict(test_imgs)\n",
        "\n",
        "  print(lh_fmri_test_pred.shape, rh_fmri_test_pred.shape)\n",
        "\n",
        "  lh_fmri_test_pred = lh_fmri_test_pred.astype(np.float32)\n",
        "  rh_fmri_test_pred = rh_fmri_test_pred.astype(np.float32)\n",
        "\n",
        "  np.save(os.path.join(args.subject_submission_dir, 'lh_pred_test.npy'), lh_fmri_test_pred)\n",
        "  np.save(os.path.join(args.subject_submission_dir, 'rh_pred_test.npy'), rh_fmri_test_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2KBex12Ll8I",
        "outputId": "da123056-6d95-47d2-9aa5-56dcfb3a2968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- Subject 1 ----------\n",
            "5/5 [==============================] - 9s 94ms/step\n",
            "5/5 [==============================] - 1s 29ms/step\n",
            "(159, 19004) (159, 20544)\n",
            "---------- Subject 2 ----------\n",
            "5/5 [==============================] - 1s 33ms/step\n",
            "5/5 [==============================] - 1s 30ms/step\n",
            "(159, 19004) (159, 20544)\n",
            "---------- Subject 3 ----------\n",
            "10/10 [==============================] - 1s 45ms/step\n",
            "10/10 [==============================] - 1s 29ms/step\n",
            "(293, 19004) (293, 20544)\n",
            "---------- Subject 4 ----------\n",
            "13/13 [==============================] - 1s 43ms/step\n",
            "13/13 [==============================] - 1s 29ms/step\n",
            "(395, 19004) (395, 20544)\n",
            "---------- Subject 5 ----------\n",
            "5/5 [==============================] - 1s 33ms/step\n",
            "5/5 [==============================] - 1s 30ms/step\n",
            "(159, 19004) (159, 20544)\n",
            "---------- Subject 6 ----------\n",
            "10/10 [==============================] - 1s 32ms/step\n",
            "10/10 [==============================] - 1s 29ms/step\n",
            "(293, 18978) (293, 20220)\n",
            "---------- Subject 7 ----------\n",
            "5/5 [==============================] - 1s 33ms/step\n",
            "5/5 [==============================] - 1s 30ms/step\n",
            "(159, 19004) (159, 20544)\n",
            "---------- Subject 8 ----------\n",
            "13/13 [==============================] - 3s 149ms/step\n",
            "13/13 [==============================] - 1s 30ms/step\n",
            "(395, 18981) (395, 20530)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Citations\n",
        "\n",
        "If you use the data provided for the Algonauts Project 2023 Challenge please cite the following papers:\n",
        "1. Gifford AT, Lahner B, Saba-Sadiya S, Vilas MG, Lascelles A, Oliva A, Kay K, Roig G, Cichy RM. 2023. *The Algonauts Project 2023 Challenge: How the Human Brain Makes Sense of Natural Scenes*. arXiv preprint, arXiv:2301.03198. DOI: https://doi.org/10.48550/arXiv.2301.03198\n",
        "\n",
        "2. Allen EJ, St-Yves G, Wu Y, Breedlove JL, Prince JS, Dowdle LT, Nau M, Caron B, Pestilli F, Charest I, Hutchinson JB, Naselaris T, Kay K. 2022. *A massive 7T fMRI dataset to bridge cognitive neuroscience and computational intelligence*. Nature Neuroscience, 25(1):116–126. DOI: https://doi.org/10.1038/s41593-021-00962-x"
      ],
      "metadata": {
        "id": "KXKAm06PpLQD"
      }
    }
  ]
}